# Contestee  
For the Contestee chatbot, my goal was to handle user queries about the app—ranging from how contests work to whether a specific event had ended. I first cleaned and structured over 100K text samples, storing them in MongoDB, and separated them into two categories: unstructured content for RAG and structured metadata like contest dates and statuses. I trained both an intent classification model and a custom NER model to route each query appropriately. If the question was more general or content-based, I used a RAG pipeline powered by T5, LLaMA 2, or GPT-3.5 to pull the answer. If the question needed precise contest data, the system used a fine-tuned model to generate the MongoDB aggregation query dynamically. The final responses were always passed through an LLM to ensure they sounded natural. This architecture significantly improved the chatbot's relevance and flexibility, making it capable of handling a wide variety of user queries in real time.

# Besty  
Besty is a comprehensive AI-powered quotation and expense management platform. I designed and developed the entire backend using FastAPI. The core AI functionality involved analyzing a company’s business inputs to either generate new quotations using GPT models or review and provide constructive feedback on manually created ones. This required thoughtful prompt engineering and logic to guide the model's output in a business-aware, financially sound way. Beyond the AI layer, I also implemented supporting features like tracking fixed and variable expenses, managing employee reimbursements, storing business-specific configurations, and ensuring all these components worked cohesively to generate accurate and explainable cost projections.

# ContentFeed  
Content Feed is a Chrome extension I built the backend for, which helps users turn any webpage they’re on into ready-to-post social media content. It starts by scraping the page content, then uses a selected LLM—GPT-4, Google Gemini, or a self-hosted LLaMA 3 model running via Ollama on a remote server I configured—to generate the output. Users can choose the content type (like a short tweet, long tweet, LinkedIn post, or Instagram carousel) and apply formatting styles. I handled the model orchestration, prompt logic, server setup for LLaMA 3, and the flexible user input pipeline. This gave users high-quality, customizable AI-generated content in real-time, right from their browser.

# Kiosk  
This was a real-time kiosk assistant deployed in an appliance showroom to help customers explore products without needing a human salesperson. I built the backend using Crew AI agents. The flow begins when a customer speaks a query like “Show me fridges.” We classify it as either a new query, a follow-up like “under 40,000,” or an exploratory one like “what is a smart feature.” Based on the intent, I then identify the relevant appliance table, fetch its metadata, and generate an SQL query dynamically. If the result set is too large, the system asks clarifying questions—like brand preference or budget range. Otherwise, it displays the filtered products. All of this happens conversationally in real-time. I handled everything from the intent-routing to SQL generation and result flow logic, except for the speech recognition part, which was handled separately.

# Summarizer  
I developed a client-facing application that could both summarize large documents and support question-answering through a RAG-based system. The input could be in any format—PDF, DOCX, or plain text. I handled the text extraction and cleaning for all formats, then fed the processed content into a vector store for retrieval-augmented generation. When users asked questions, the system would pull relevant chunks and pass them to GPT to generate accurate responses. I also built a separate summarization flow for long documents, allowing clients to quickly get a high-level overview. The entire system was optimized to respond in under a second, even for large files, providing a smooth and responsive user experience.
