# ‚úÖ Chapter 1: Introduction to MLflow
## üéØ What is MLflow?
MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It helps you keep track of experiments, package ML code, reproduce runs, share models, and deploy them‚Äîall in one place.

It supports any ML library (scikit-learn, PyTorch, TensorFlow, XGBoost, etc.) and is language-agnostic, although it has strong Python support.

## üöÄ Why Use MLflow in ML Projects?
### Without MLflow:
- Your experiments are scattered across notebooks or folders.
- Difficult to reproduce which model was trained with what parameters.
- Manual tracking of metrics in spreadsheets.
- Deployment involves custom scripts.

### With MLflow:
- Track parameters, metrics, models, and outputs automatically.
- Compare runs visually via a web UI.
- Package and share your code in reusable format.
- Easily deploy or serve models via API.
- Centralized Model Registry to manage multiple versions.

MLflow simplifies collaboration, reproducibility, and production-readiness in ML workflows.

## üß© Overview of MLflow Components
MLflow consists of four major components:

| Component | Description |
|---|---|
| MLflow Tracking | Record and query experiments: parameters, metrics, artifacts, and logs. |
| MLflow Projects | Package your code with environment details for reproducibility. |
| MLflow Models | Format to save/load ML models with multiple deployment options. |
| Model Registry | Central store to manage models, versioning, stage transitions, approvals. | 

We'll go deeper into each component in later chapters.

## üõ†Ô∏è Installation and Setup
You can install MLflow via pip:

```python
pip install mlflow
```
To verify installation and check the version:

```bash
mlflow --version
```
MLflow also runs a local server for experiment tracking. Start it using:

```bash
mlflow ui
```
This starts a web server on http://localhost:5000 by default.

‚úÖ First Hello World Experiment with MLflow
Let‚Äôs now do a simple machine learning experiment using scikit-learn and track everything with MLflow.

üìÇ Folder Structure
```bash
mlflow_hello_world/
‚îÇ
‚îú‚îÄ‚îÄ train.py

```
## üß™ Step-by-Step: Code Explanation
Create a file named train.py with the following code:

```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Load Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Enable automatic logging (optional)
mlflow.sklearn.autolog()

# Start an MLflow run
with mlflow.start_run():

    # Define model and train
    model = RandomForestClassifier(n_estimators=100, max_depth=3)
    model.fit(X_train, y_train)

    # Predict
    predictions = model.predict(X_test)
    acc = accuracy_score(y_test, predictions)

    # Manually log metrics (optional if autolog is used)
    mlflow.log_param("n_estimators", 100)
    mlflow.log_param("max_depth", 3)
    mlflow.log_metric("accuracy", acc)

    # Log model artifact
    mlflow.sklearn.log_model(model, "model")

    print("Logged to MLflow")


```
## ‚ñ∂Ô∏è Run the Script
```python
python train.py
```
This will:

- Create an experiment
- Log the run
- Save metrics, parameters, and the trained model

## üîé Visualize in the UI
Start the MLflow tracking UI:

```bash
mlflow ui
```
Visit: http://localhost:5000

You‚Äôll see:

- Each run listed
- Parameters like n_estimators, max_depth
- Metrics like accuracy
- Artifacts (model, etc.)
- Source code snapshot

## üì¶ Artifacts and Files
After a run, MLflow stores metadata and model files in a default directory:
```bash
mlruns/
‚îî‚îÄ‚îÄ 0/
    ‚îî‚îÄ‚îÄ <run_id>/
        ‚îú‚îÄ‚îÄ metrics/
        ‚îú‚îÄ‚îÄ params/
        ‚îú‚îÄ‚îÄ artifacts/
        ‚îî‚îÄ‚îÄ meta.yaml
```
# üß† Key Concepts Recap
- Term	Meaning
- Run	A single experiment execution
- Experiment	Group of related runs
- Artifact	Output files like models or plots
- Tracking URI	The destination (local/remote) where data is logged

---

# üìò Chapter 2: Experiment Tracking with MLflow Tracking

---

## üéØ Goal of This Chapter

By the end of this chapter, you will understand how to:
- Log **parameters**, **metrics**, and **artifacts** using MLflow
- Use **MLflow UI and CLI** to inspect and compare experiments
- Enable **automatic logging** with popular ML frameworks
- **Organize** experiments and runs for better traceability

---

## üß© 1. Logging Parameters, Metrics, and Artifacts

MLflow Tracking API allows you to log:
- **Parameters**: Model or experiment configuration (e.g., learning rate, depth)
- **Metrics**: Performance indicators (e.g., accuracy, loss)
- **Artifacts**: Files generated by the run (e.g., models, plots, logs)

### ‚úÖ Logging Manually

```python
import mlflow
import mlflow.sklearn

mlflow.start_run()

mlflow.log_param("learning_rate", 0.01)
mlflow.log_metric("accuracy", 0.96)

# Log a model
mlflow.sklearn.log_model(trained_model, "model")

# Log an artifact (e.g., a plot)
mlflow.log_artifact("confusion_matrix.png")

mlflow.end_run()
```

You can also use `with` syntax:

```python
with mlflow.start_run():
    # do training...
    mlflow.log_param(...)
    mlflow.log_metric(...)
```

---

## üñºÔ∏è 2. Comparing Runs Visually in MLflow UI

Start MLflow UI:

```bash
mlflow ui
```

Open `http://localhost:5000`

### Inside the UI:
- Click on an **experiment name**
- Compare **runs** side-by-side
- See **parallel coordinates**, **scatter plots**, and metric trends
- Download logged files/artifacts

This is extremely useful when tuning hyperparameters across multiple runs.

---

## ‚öôÔ∏è 3. Automatic Logging with Popular ML Libraries

MLflow supports **autologging** for:

| Library        | Method                  |
|----------------|--------------------------|
| scikit-learn   | `mlflow.sklearn.autolog()` |
| TensorFlow     | `mlflow.tensorflow.autolog()` |
| PyTorch Lightning | `mlflow.pytorch.autolog()` |
| XGBoost        | `mlflow.xgboost.autolog()` |
| LightGBM       | `mlflow.lightgbm.autolog()` |

### Example:

```python
import mlflow.sklearn
mlflow.sklearn.autolog()

with mlflow.start_run():
    model.fit(X_train, y_train)
```

This logs:
- Parameters
- Metrics
- Model structure
- Artifacts
- Feature importance (when supported)

No manual logging required!

---

## üß™ 4. Using MLflow CLI

You can interact with experiments using MLflow‚Äôs **command-line interface**:

### List Experiments:

```bash
mlflow experiments list
```

### Create a New Experiment:

```bash
mlflow experiments create -n "My New Experiment"
```

### Run a Project (if using MLproject):

```bash
mlflow run . -P alpha=0.5
```

### Get Info About a Run:

```bash
mlflow runs list
mlflow runs describe <run_id>
```

You can also **delete** or **restore** experiments from the CLI.

---

## üóÉÔ∏è 5. Organizing Experiments and Runs

By default, MLflow logs all runs under a **default experiment** named `"Default"`.

To organize better:

### Create and Use Custom Experiments:

```python
mlflow.set_experiment("iris_classifier_dev")
```

This creates the experiment if it doesn‚Äôt exist.

### Nested Runs:

You can also track nested experiments (e.g., for cross-validation):

```python
with mlflow.start_run() as parent_run:
    for fold in range(5):
        with mlflow.start_run(nested=True):
            # Train and log each fold
            ...
```

### Set Tags:

Add metadata to your runs for filtering:

```python
mlflow.set_tag("developer", "Shubhendu")
mlflow.set_tag("stage", "dev")
```

In the UI, you can **filter** runs based on tags like `stage = 'prod'`.

---

## üß† Summary

| Feature                 | Use                                                             |
|-------------------------|------------------------------------------------------------------|
| `log_param()`           | Save model hyperparameters                                       |
| `log_metric()`          | Save evaluation metrics like accuracy, loss                     |
| `log_artifact()`        | Save files (plots, models, etc.)                                |
| `autolog()`             | Automatically log everything from supported libraries           |
| `set_experiment()`      | Create or switch to a named experiment                          |
| `set_tag()`             | Add metadata for better filtering and organization              |
| **MLflow UI**           | Visually compare runs, see metrics, artifacts                   |
| **MLflow CLI**          | Manage experiments and runs from the terminal                   |

---

## üß™ Optional Exercise

Try modifying your `train.py` from Chapter 1:
- Enable `autolog()`
- Run the script multiple times with different `max_depth` values
- Use `set_experiment("iris_rf_experiment")`
- View and compare runs in the UI

---

‚úÖ **Next Up**: Chapter 3 ‚Äî Packaging ML Code with MLflow Projects (How to make your ML code reproducible and portable)

```

